---

# Disable it for now till we figure out the exact firewall configuration that is needed.
- name: Configure firewall
  become: True
  systemd:
    name: firewalld
    state: stopped
    enabled: no

- name: Run kubeadm init
  become: True
  shell: "kubeadm init --pod-network-cidr=\"{{ pod_network_cidr }}\" --apiserver-advertise-address=\"{{ master_ip }}\" --apiserver-cert-extra-sans=\"{{ master_ip }}\" --node-name master-1  "

- name: Print the join command
  become: True
  shell: kubeadm token create --print-join-command
  register: token_create_output


- name: Set join command as fact
  set_fact: 
    kubeadm_join_command: "{{ token_create_output.stdout }}"


- name: Create the .kube directory
  file:
    path: /home/vagrant/.kube
    state: directory
    mode: 0755
    owner: vagrant
    group: vagrant


- name: Copy kube config to .kube
  copy:
    src: /etc/kubernetes/admin.conf
    dest: /home/vagrant/.kube/config
    remote_src: True
    owner: vagrant
    group: vagrant
    mode: 0644
  become: True

- name: Install calico
  shell: kubectl apply -f https://docs.projectcalico.org/v3.7/manifests/calico.yaml


- name: Copy the python script to check pod status
  copy:
    src: check_pod_status.py
    dest: /tmp/check_pod_status.py
    owner: vagrant
    group: vagrant
    mode: 0644

- name: Wait for the pods to be ready
  shell: "kubectl get pods --all-namespaces -o json | python /tmp/check_pod_status.py >> /tmp/output.txt"
  register: pod_status
  retries: 20
  delay: 15
  until: pod_status.rc == 0

- name: Remove the python script to check pod status
  file:
    state: absent
    path: "{{ item }}"
  with_items:
    - /tmp/check_pod_status.py
    - /tmp/output.txt


- name: Install the kubernetes dashboard
  shell: kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml


- name: Create an admin account called k8s-admin
  shell: kubectl --namespace kube-system create serviceaccount k8s-admin


- name: create clusterrolebinding k8s-admin
  shell: kubectl create clusterrolebinding k8s-admin --serviceaccount=kube-system:k8s-admin --clusterrole=cluster-admin


- name: Install kubectl service
  copy:
    src: kubeproxy.service
    dest: /usr/lib/systemd/system/kubeproxy.service
    owner: root
    group: root
    mode: 0644
  become: True
  become_user: root


- name: Start the kubectl service
  systemd:
    state: started
    enabled: True
    daemon_reload: yes
    name: kubeproxy
  become: True


- name: Download helm package manager
  unarchive:
    src: https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz
    dest: /tmp
    remote_src: yes

- name: Move helm and tiller binaries to /usr/local/bin
  copy:
    src: "/tmp/linux-amd64/{{ item }}"
    dest: /usr/local/bin
    remote_src: True
    owner: root
    group: root
    mode: 0755
  become: True
  with_items:
    - helm
    - tiller

- name: Remove the unarchived helm package folder from temp
  file:
    state: absent
    path: /tmp/linux-amd64


- name: Initialize helm 
  shell: helm init --upgrade

